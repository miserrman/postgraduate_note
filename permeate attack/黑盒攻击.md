# 黑盒攻击

## 基于白盒的迁移攻击



## 基于梯度的攻击

### ZOO:Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models

核心思想：利用正负扰动带来的概率差估算一阶导（梯度）和二阶导，再利用ADAM或者牛顿法等方法更新x。本质为通过估算梯度将黑盒转换为白盒过程。重要性采样也颇为亮眼。

用牛顿迭代法解[非线性方程](https://baike.baidu.com/item/非线性方程/4778289)，是把非线性方程

![img](https://bkimg.cdn.bcebos.com/formula/7a492ae6b0501ded19000b6a25e5c2bc.svg)

 线性化的一种近似方法。把

![img](https://bkimg.cdn.bcebos.com/formula/ff00f7b7211e5b5454eccf5df11db931.svg)

 在点

![img](https://bkimg.cdn.bcebos.com/formula/df1b580a63b720439fa2588433680d70.svg)

 的某邻域内展开成泰勒级数

![img](https://bkimg.cdn.bcebos.com/formula/b13cf29677168fc389dc3e0ea3f6ef79.svg)

 ，取其线性部分（即泰勒展开的前两项），并令其等于0，即

![img](https://bkimg.cdn.bcebos.com/formula/a96b495e7a5799e170865bb7a8f9261d.svg)

 ，以此作为非线性方程

![img](https://bkimg.cdn.bcebos.com/formula/7a492ae6b0501ded19000b6a25e5c2bc.svg)

 的近似方程，若

![img](https://bkimg.cdn.bcebos.com/formula/43aeadb536441e4875fa94493bc1091e.svg)

 ，则其解为

![img](https://bkimg.cdn.bcebos.com/formula/c4f23f58bd4c0e17be502b9b314ce2af.svg)

 ， 这样，得到牛顿迭代法的一个迭代关系式：

![img](https://bkimg.cdn.bcebos.com/formula/3da0627fe7bf6f8bb31102cc5149f651.svg)

 。对于之前我们解最大似然估计使用了梯度下降法，这边我们使用牛顿法，速度更快。

牛顿法也就是要求解[![clip_image002](https://images0.cnblogs.com/blog/311205/201305/18230054-e8ab25299be04d839c47844dcd88ae48.gif)](https://images0.cnblogs.com/blog/311205/201305/18230053-9b77cf2555824b6b8361afbed70465d2.gif)，[![clip_image004](https://images0.cnblogs.com/blog/311205/201305/18230055-3f7a341c354a4c10ae9b223757b03549.gif)](https://images0.cnblogs.com/blog/311205/201305/18230054-67ab81b0989e42aa8008caae856116db.gif)可导，θ用下面进行迭代。

[![clip_image006](https://images0.cnblogs.com/blog/311205/201305/18230056-ff5eea6a164b41ceb82dc7e9fc94e1d2.gif)](https://images0.cnblogs.com/blog/311205/201305/18230055-cabec532f1ea408bb4129d91c06cb8dc.gif)

具体看这个图

[![clip_image008](https://images0.cnblogs.com/blog/311205/201305/18230056-783c62b8310a499a8156405aed63cdda.gif)](https://images0.cnblogs.com/blog/311205/201305/18230056-7025f7d1e4dd44988d2919845e812ec7.gif)

[![clip_image010](https://images0.cnblogs.com/blog/311205/201305/18230059-52fa4030daa649b9a27c537fe8d2c656.gif)](https://images0.cnblogs.com/blog/311205/201305/18230059-ac23c03bc48e4206ad885f1b67c7b7f6.gif)

对于我们刚刚的求最大似然估计，也就是[![clip_image012](https://images0.cnblogs.com/blog/311205/201305/18230100-275e84eb5843437d975574ccb80516b3.gif)](https://images0.cnblogs.com/blog/311205/201305/18230100-17330c33da0949f482fda7a3d62af84c.gif)，则

[![clip_image014](https://images0.cnblogs.com/blog/311205/201305/18230101-d48db0f22777446bae7149c4b0630e3a.gif)](https://images0.cnblogs.com/blog/311205/201305/18230100-2574363a36204aa4b806459a85ced0ba.gif)

下面在原理上说一说。

摘自：http://blog.csdn.net/luoleicn/article/details/6527049

对于一个目标函数f，求函数f的极大极小问题，可以转化为求解函数f的导数f'=0的问题，这样求可以把优化问题看成方程求解问题（f'=0），为了求解f'=0的根，把f（x）的泰勒展开，展开到2阶形式：

[![clip_image015](https://images0.cnblogs.com/blog/311205/201305/18230101-e2de3f39f1eb4ef0a2577930229689b4.gif)](https://images0.cnblogs.com/blog/311205/201305/18230101-5eafd420cffe463a8c719e9d487dbc00.gif)

这个式子是成立的，当且仅当 Δ*x* 无线趋近于0。此时上式等价与：

[![clip_image016](https://images0.cnblogs.com/blog/311205/201305/18230102-fa97ca8a6b29424eb9f2a10166b385dc.gif)](https://images0.cnblogs.com/blog/311205/201305/18230101-5a2ec9aeb03744fe874eec2c3d8fea75.gif)

求解：

[![clip_image017](https://images0.cnblogs.com/blog/311205/201305/18230102-805f2da72ea540ac9114e3f388487c6c.gif)](https://images0.cnblogs.com/blog/311205/201305/18230102-9b9ff8aef8ad45439069f88d0bad10bc.gif)

得出迭代公式：

[![clip_image018](https://images0.cnblogs.com/blog/311205/201305/18230103-c9f7e2972b8447868c920fa9790e2a95.gif)](https://images0.cnblogs.com/blog/311205/201305/18230103-3b0f53e0771948ce88f840c09cde688e.gif)

牛顿法可以快速得到最优

本文直接提出一种新方法，使用零阶优化（ZOO）来直接估计目标模型的梯度：

数学优化中有一个分支叫做无导数优化（有很多别名：DFO，无梯度优化，零次优化，gradient-free，derivative-free等）

虽然不能获取梯度值，但可以用取极限的方法（即导数的定义）来获得一个对梯度的估计，这是允许的。

首先我们先对输入 x  进行一个扰动 x = x + h 其中 h = 0.0001 是一个常量值，e 是一个标准单位向量，你可以理解为某一位为1其余都是0的向量。最后使用对称差商(symmetric difference quotient )来估计梯度

![img](https://img-blog.csdnimg.cn/f4bb980534fd42d2be3a94d9d4bf525f.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5YeJ6Iy2aQ==,size_16,color_FFFFFF,t_70,g_se,x_16)

 在增加一次查询就能得到二阶信息

![img](https://img-blog.csdnimg.cn/5dc27975f2a74ea3878abc002f09b47b.png?x-oss-process=image/watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBA5YeJ6Iy2aQ==,size_19,color_FFFFFF,t_70,g_se,x_16)

 有了这两个梯度估计值，就可以直接对 x 进行梯度下降优化了。比如牛顿法

## 基于Gan的攻击

